{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from Abbrev import ABBREVIATIONS\n",
    "from contractions import CONTRACTION_MAP\n",
    "from wordfreq import WordFrequency\n",
    "\n",
    "\n",
    "import pandas.io.sql as pds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from sklearn.manifold import TSNE\n",
    "from pattern3.en import tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from __future__ import print_function\n",
    "from sklearn.manifold import MDS\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "#import func\n",
    "import importlib\n",
    "#importlib.reload(func)\n",
    "\n",
    "def connect(query):\n",
    "    conn_str = (\n",
    "    r'DRIVER={ODBC Driver 13 for SQL Server};'\n",
    "    r'SERVER=ICWDSADASCI01VG;'\n",
    "    r'DATABASE=CernerNotes;'\n",
    "    r'Trusted_Connection=yes;')\n",
    "    cnxn = pyodbc.connect(conn_str)\n",
    "    #query ='\"\"\"{}\"\"\"'.format(query)\n",
    "    query =query \n",
    "    df = pds.read_sql(query, cnxn)\n",
    "    return df\n",
    "    \n",
    "def Dataframe(corpus):\n",
    "    for x in corpus:\n",
    "        dfNotes = corpus['NoteText']\n",
    "        dfID=corpus['EVENT_ID']\n",
    "    return dfID, dfNotes\n",
    "\n",
    "def Dataframe(corpus):\n",
    "    dfID = [x for x in corpus['EVENT_ID']]\n",
    "    dfNotes = [x for x in corpus['NoteText']]\n",
    "   \n",
    "    return dfID, dfNotes\n",
    "\n",
    "def Transform(EVID,note=False,cluster=False):\n",
    "    Model = pd.DataFrame({'EventID': EVID})\n",
    "    Model.insert(0, 'Order_ID', range(1, 1+ len(Model)))\n",
    "    ModelID = Model['Order_ID']\n",
    "    ID = ModelID.tolist()\n",
    "\n",
    "    if note:\n",
    "\n",
    "        vector = { 'Event': ID,'EventID':EVID,'NoteText': note }\n",
    "        frame = pd.DataFrame(vector, index = [ID] , columns = ['Event','EventID','NoteText'])\n",
    "        return vector,frame\n",
    "    elif cluster:\n",
    "\n",
    "        vector = { 'Event': ID,'cluster': cluster }\n",
    "        frame = pd.DataFrame(vector, index = [cluster] , columns = ['Event','cluster'])\n",
    "        return vector,frame,ID\n",
    "    else:\n",
    "        vector = { 'Event': ID,'EventID':EVID}\n",
    "        frame = pd.DataFrame(vector, index = [ID] , columns = ['Event','EventID'])\n",
    "        return vector,frame\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get',\n",
    "                                 'tell', 'listen', 'one', 'two', 'three',\n",
    "                                 'four', 'five', 'six', 'seven', 'eight',\n",
    "                                 'nine', 'zero', 'join', 'find', 'make',\n",
    "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
    "                                 'also']\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def remove_generic_text(note):\n",
    "    clean = re.sub(\n",
    "    '(Tel)*:*\\s\\(*\\d{3}\\)*\\s*-*\\d{3}-*\\s*\\d{4}\\s|(Fax)*:*\\s\\(*\\d{3}\\)*\\s*-*\\d{3}-*\\s*\\d{4}\\s|Med\\.\\sRec\\.\\s#\\:\\s[0-9]{2,13}|(DOB)*:*\\s*\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}\\s*|MRN:\\s\\d{1,8}|FIN:\\s\\d{1,12}|Accession #:\\s[a-zA-Z0-9]{2,5}-[a-zA-Z0-9]{2,5}|Patient: [A-Za-z]{1,10}, [A-Za-z]{1,10}|Encounter\\s#\\:\\s[0-9]{2,15}|Age: \\d{1,3}|Sex: [A-Za-z]{1,10}| Gender: [A-Za-z]{1,10}|Patient Name: [A-Za-z]{1,10}, [A-Za-z]{1,10}|Author: [A-Za-z]{2,25}, [A-Za-z]{2,25}',\" \",note)\n",
    "    return clean\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def replace_abbreviation(text):\n",
    "    xxx=[]\n",
    "    for line in text:\n",
    "        new_line = line\n",
    "        for word in line.split():\n",
    "            abb = ABBREVIATIONS.get(word)\n",
    "            #for f_key, f_value in ABBREVIATIONS.items():\n",
    "            if abb is not None:\n",
    "                new_line = new_line.replace(word,abb)\n",
    "    xxx.append(new_line)\n",
    "    print(xxx)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "            \n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z0-9]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def normalize_corpus(corpus,tokenize=False):\n",
    "    \n",
    "    normalized_corpus = []    \n",
    "    for text in corpus:\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        text = remove_generic_text(text)  \n",
    "        #text = replace_abbreviation(text)\n",
    "        text = text.lower()\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        \n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "        else:\n",
    "            normalized_corpus.append(text)\n",
    "\n",
    "    return normalized_corpus   \n",
    "\n",
    "# def build_feature_matrix(documents, feature_type='tfidf', ngram_range=(1, 1), min_df=0.0, max_df=1.0,use_idf=False):\n",
    "\n",
    "#     feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "#     if feature_type == 'binary':\n",
    "#         vectorizer = CountVectorizer(binary=True, min_df=min_df,\n",
    "#                                      max_df=max_df, ngram_range=ngram_range)\n",
    "#     elif feature_type == 'frequency':\n",
    "#         vectorizer = CountVectorizer(binary=False, min_df=min_df,\n",
    "#                                      max_df=max_df, ngram_range=ngram_range)\n",
    "#     elif feature_type == 'tfidf':\n",
    "#         vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, \n",
    "#                                      ngram_range=ngram_range,use_idf=use_idf)\n",
    "#     else:\n",
    "#         raise Exception(\"Possible values: 'binary', 'frequency', 'tfidf'\")\n",
    "\n",
    "#     feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    \n",
    "#     return vectorizer, feature_matrix\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(query_TFIDF, corpus_TFIDF, top_n=3):\n",
    "    similarity = cosine_similarity(query_TFIDF, corpus_TFIDF).flatten()\n",
    "    # get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(similarity[index], 3))for index in top_docs]\n",
    "    return top_docs_with_score\n",
    "\n",
    "def Tokenize_norm(note):\n",
    "    toks = []\n",
    "    for x in note:\n",
    "        token = tokenize_text(x)\n",
    "    toks.append(token)\n",
    "    return toks\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_extractor(documents,ngram_range, min_df,max_df, use_idf=False):\n",
    "      \n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, min_df=min_df)\n",
    "    features = vectorizer.fit_transform(documents)\n",
    "    f_names = vectorizer.get_feature_names()        \n",
    "    return vectorizer, features, f_names\n",
    "\n",
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features, columns=feature_names)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer,bow_features,features_names = bow_extractor(norm_corpusTrain,ngram_range =(0,1),min_df =0.2,max_df=0.8)\n",
    "test_features = bow_vectorizer.transform(norm_corpusTest)\n",
    "test_features = test_features.todense()\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames=bow_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_features(tf,features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in features_names:\n",
    "#     for line in note:\n",
    "#         for word in line.split():\n",
    "#             wf = WordFrequency.get(word)\n",
    "#             if wf is not None:\n",
    "#                 tfidf = tf * idf + wf\n",
    "#             else: tfidf = tf * idf\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the document frequency matrix\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df # to smoothen idf later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute inverse document frequencies\n",
    "total_docs = 1 + len(norm_corpusTrain)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = bow_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d#isplay_features(feature, features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # compute idf diagonal matrix  \n",
    "total_features = bow_features.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf = idf_diag.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute tfidf feature matrix\n",
    "tfidf = tf * idf\n",
    "#compute L2 norms \n",
    "norms = norm(tfidf, axis=1)\n",
    "# compute normalized tfidf\n",
    "norm_Train_tfidf = tfidf / norms[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.05426927 ... 0.         0.0250985  0.02793408]\n",
      " [0.01745629 0.04407441 0.0623576  ... 0.         0.03460704 0.01283896]\n",
      " [0.19081654 0.         0.1363275  ... 0.         0.         0.04678134]\n",
      " ...\n",
      " [0.         0.15026577 0.         ... 0.04773245 0.05243913 0.0583636 ]\n",
      " [0.         0.         0.09013458 ... 0.         0.04168552 0.        ]\n",
      " [0.05614869 0.         0.04011503 ... 0.         0.03710487 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(norm_Train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tfidf using idf matrix from train corpus\n",
    "test_tfidf = test_features*idf\n",
    "test_norms = norm(test_tfidf, axis=1)\n",
    "Test_tfidf = test_tfidf / test_norms[:, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = connect(\"\"\"SELECT TOP (300) EVENT_ID, NoteText FROM [CernerNotes].[dbo].[DecompressedNote] where NoteText LIKE N'%Breast C_%' OR NoteText LIKE N'%Metastatic Breast Cancer%' OR NoteText LIKE N'%Triple-Negative%' OR NoteText LIKE N'%tnbc%' AND(len([NoteText]) - LEN(replace([NoteText], ' ', '')) +1)<200  AND (len([NoteText]) - LEN(replace([NoteText], ' ', '')) +1)>180\"\"\")\n",
    "dfTest = connect(\"\"\"SELECT TOP (50) EVENT_ID, NoteText FROM [CernerNotes].[dbo].[DecompressedNote]  WHERE (len([NoteText]) - LEN(replace([NoteText], ' ', '')) +1)<200  AND (len([NoteText]) - LEN(replace([NoteText], ' ', '')) +1)>180\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIDtr, dfNotestr =Dataframe(dfTrain)\n",
    "dfIDtest, dfNotest =Dataframe(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectTrain,frameTrain = Transform(dfIDtr,dfNotestr)\n",
    "vectTest,frameTest = Transform(dfIDtest,dfNotest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpusTrain = normalize_corpus(dfNotestr,tokenize=False)\n",
    "norm_corpusTest = normalize_corpus(dfNotest,tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "train_common = unique_tokens(norm_corpusTrain)\n",
    "test_common = unique_tokens(norm_corpusTest)\n",
    "tr = unique(train_common)\n",
    "ts = unique(test_common)\n",
    "joint = tr + ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_u = unique(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile = \"C:/Users/oolakunle/Desktop/output.csv\"\n",
    "\n",
    "#Assuming res is a flat list\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in joint_u:\n",
    "        writer.writerow([val])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer, tfidf_features = build_feature_matrix(norm_corpusTrain,feature_type='tfidf',ngram_range=(1, 3), min_df=0.0, max_df=1.0)\n",
    "# Test_tfidf = tfidf_vectorizer.transform(norm_corpusTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer, tfidf_features = build_feature_matrix(norm_corpusTrain,norm_corpusTest,ngram_range=(0, 1), min_df=1)\n",
    "# Test_tfidf = tfidf_vectorizer.transform(norm_corpusTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Cosine similarity results for our example documents\n",
    "sim = []\n",
    "disim = []\n",
    "print('Document Similarity Analysis using Cosine Similarity')\n",
    "print ('='*60)\n",
    "\n",
    "for index, E in enumerate(frameTest['EventID']):\n",
    "    doc_tfidf = Test_tfidf[index]\n",
    "    top_similar_docs = compute_cosine_similarity(doc_tfidf,norm_Train_tfidf, top_n=1)\n",
    "    print ('Document Number',index+1 ,'With EventID:', E ) \n",
    "    print ('Top', len(top_similar_docs), 'similar docs:')\n",
    "    print( '-'*40)\n",
    "\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "         for A, B in enumerate(frameTrain['EventID']):     \n",
    "                if doc_index+1 == A:\n",
    "\n",
    "                    if sim_score<=1.00 and sim_score>0.10:\n",
    "\n",
    "                            print('Similar')\n",
    "                            print('Doc num: {} Similarity Score: {}\\nDoc num: {}'.format(doc_index+1,sim_score,B))\n",
    "                            sim.append(B)\n",
    "\n",
    "                    elif sim_score<=0.10:\n",
    "                        #if doc_index+1 == A:\n",
    "                            print('No Similarity')\n",
    "                            print('Doc num: {} Similarity Score: {}\\nDoc num: {}'.format(doc_index+1,sim_score,B))\n",
    "                            disim.append(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = connect(\"\"\"SELECT EVENT_ID, NoteText FROM [CernerNotes].[dbo].[DecompressedNote] where EVENT_ID =14009026858\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2  = connect(\"\"\"SELECT EVENT_ID, NoteText FROM [CernerNotes].[dbo].[DecompressedNote] where EVENT_ID =2505100836\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp1_EID,df_samp1_Notes = Dataframe(sample_1)\n",
    "df_samp2_EID,df_samp2_Notes = Dataframe(sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp1_Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp2_Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = normalize_corpus(df_samp1_Notes)\n",
    "s2 = normalize_corpus(df_samp2_Notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_tokens(normalized):\n",
    "    toks = Tokenize_norm(normalized)\n",
    "    tokens = [item for sublist in toks for item in sublist]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(tokens):\n",
    "    fdist=FreqDist(tokens)\n",
    "    most_common = fdist.most_common()\n",
    "    unique = []\n",
    "    for x,y in most_common:\n",
    "        if y>=1:\n",
    "            unique.append(x)\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss1 = Tokenize_norm(s1)\n",
    "ss2 =Tokenize_norm(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = [item for sublist in ss1 for item in sublist]\n",
    "tokens2 = [item for sublist in ss2 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "fdist1=FreqDist(tokens1)\n",
    "fdist2=FreqDist(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common1 = fdist1.max()\n",
    "most_common2 = fdist2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fdist1.most_common()\n",
    "b = fdist2.most_common()\n",
    "#print(a, end= \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(b, end= \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = []\n",
    "for x,y in a:\n",
    "    if y>=1:\n",
    "        arr1.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = []\n",
    "for x,y in b:\n",
    "    if y>=1:\n",
    "        arr2.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common= [x for x in arr1 if x in arr2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common, end= \"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = round((len(tokens2)/len(tokens1))*100)\n",
    "# z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = round((len(common)/len(tokens1))*100)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = round((len(common)/len(tokens2))*100)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization with MDS\n",
    "totalvocab_tokenized = []\n",
    "for i in norm_corpusTest:\n",
    "    allwords_tokenized = tokenize_text(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe with EventID as the Index\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer, feature_matrix =build_feature_matrix(norm_corpusTest, feature_type='tfidf', ngram_range=(1, 4), min_df=0.2, max_df=0.8, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance Using Cosine Similarity\n",
    "dist = 1 - cosine_similarity(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating clusters using kmeans\n",
    "num_clusters = 10\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(feature_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector,frame,ID = Transform(dfIDtest,False,clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(num_clusters):\n",
    "    \n",
    "    print(\"Words common to cluster %d:   \" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        v = vocab_frame.loc[terms[ind].split(' ')].values.tolist()\n",
    "        #x=v.astype(np.float)\n",
    "        #x = np.array(v, dtype=np.float32)\n",
    "        ##mv\n",
    "        print(' %s' %v[0][0], end=',')\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Notes in cluster %d:  \" % i, end='')\n",
    "    for Event in frame.loc[i]['Event'].values.tolist():\n",
    "        print('%s,' % Event, end='')\n",
    "    print()\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multidimensional scaling\n",
    "MDS()\n",
    "\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e', 5: '#800080', 6: '#000080', 7: '#00FFFF', 8: '#FF0000', 9: '#FFFF00'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: '1', 1: '2', 2: '3',3: '4', 4: '5',5: '6',6: '7',7: '8',8: '9',9: '10'}\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=ID)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(20, 12)) # set size\n",
    "ax.margins(0.12) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=8, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='False',      # ticks along the bottom edge are off\n",
    "        top='False',         # ticks along the top edge are off\n",
    "        labelbottom='False')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='False',      # ticks along the bottom edge are off\n",
    "        top='False',         # ticks along the top edge are off\n",
    "        labelleft='False')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['title'], size=12)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfNotest[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfNotest[71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal Component Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
